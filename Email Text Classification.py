# -*- coding: utf-8 -*-
"""Task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eu0IG3StfZ0PBVeLxLkyYEBVHWA_c8b_
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
import torch
from torch.utils.data import Dataset, DataLoader

# Read the CSV file
df = pd.read_csv('subject_categorized.csv')

# Dropping any duplicates
df.drop_duplicates(inplace=True)

# Handling missing values (if any)
df.dropna(inplace=True)

# Text preprocessing (lowercasing, removing punctuation, etc.)
df['subject'] = df['subject'].str.lower()
df['subject'] = df['subject'].str.replace('[^\w\s]', '')

# Split the dataset into train, validation, and test sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)

# Get the subject and collection columns as separate lists
train_subjects = train_df['subject'].tolist()
train_collections = train_df['collection'].tolist()

val_subjects = val_df['subject'].tolist()
val_collections = val_df['collection'].tolist()

test_subjects = test_df['subject'].tolist()
test_collections = test_df['collection'].tolist()

# Perform label encoding on the collections
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_collections)
val_labels = label_encoder.transform(val_collections)
test_labels = label_encoder.transform(test_collections)

# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Tokenize the subjects for training set
train_tokenized = tokenizer.batch_encode_plus(
    train_subjects,
    max_length=64,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    return_tensors='pt'
)

train_input_ids = train_tokenized['input_ids']
train_attention_masks = train_tokenized['attention_mask']

# Tokenize the subjects for validation set
val_tokenized = tokenizer.batch_encode_plus(
    val_subjects,
    max_length=64,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    return_tensors='pt'
)

val_input_ids = val_tokenized['input_ids']
val_attention_masks = val_tokenized['attention_mask']

# Tokenize the subjects for test set
test_tokenized = tokenizer.batch_encode_plus(
    test_subjects,
    max_length=64,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    return_tensors='pt'
)

test_input_ids = test_tokenized['input_ids']
test_attention_masks = test_tokenized['attention_mask']

class CustomDataset(Dataset):
    def __init__(self, input_ids, attention_masks, labels):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        input_id = self.input_ids[idx]
        attention_mask = self.attention_masks[idx]
        label = self.labels[idx]

        return {
            'input_ids': input_id,
            'attention_mask': attention_mask,
            'labels': label
        }

# Define batch size and create dataloaders
batch_size = 16

train_dataset = CustomDataset(train_input_ids, train_attention_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

val_dataset = CustomDataset(val_input_ids, val_attention_masks, val_labels)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size)

test_dataset = CustomDataset(test_input_ids, test_attention_masks, test_labels)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

# Define the number of labels
num_epochs = 10
num_labels = len(label_encoder.classes_)

# Load the pre-trained BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)

# Set the device to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Define the optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=2e-5)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)
# Set the number of training epochs

# Training loop
for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        # Move batch to device
        inputs = {key: value.to(device) for key, value in batch.items()}

        # Forward pass
        outputs = model(**inputs)
        loss = outputs.loss

        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # Update weights
        optimizer.step()
        scheduler.step()
        model.zero_grad()

model.eval()
val_loss = 0
val_correct = 0
val_total = 0

with torch.no_grad():
    for batch in val_dataloader:
        inputs = {key: value.to(device) for key, value in batch.items()}

        # Forward pass
        outputs = model(**inputs)
        loss = outputs.loss
        logits = outputs.logits

        # Compute validation loss
        val_loss += loss.item() * batch_size

        # Compute accuracy
        _, predicted_labels = torch.max(logits, dim=1)
        val_correct += (predicted_labels == inputs['labels']).sum().item()
        val_total += len(inputs['labels'])

    # Average validation loss and accuracy
    avg_val_loss = val_loss / val_total
    val_accuracy = val_correct / val_total * 100

print("Validation Loss: {:.4f}".format(avg_val_loss))
print("Validation Accuracy: {:.2f}%".format(val_accuracy))

# Specify the path where you want to save the model
model_path = 'model.pt'

# Save the model's state dictionary
torch.save(model.state_dict(), model_path)

# Load the saved model's state dictionary
model_path='/content/model.pt'
loaded_state_dict = torch.load(model_path)

# Create a new instance of the model
loaded_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)

# Load the state dictionary into the model
loaded_model.load_state_dict(loaded_state_dict)

# Set the device to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
loaded_model.to(device)

# Prepare the input text
text = "Showcase your expertise with powerful demos"
# Tokenize the input text
inputs = tokenizer.encode_plus(
    text,
    max_length=64,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    return_tensors='pt'
)

input_ids = inputs['input_ids'].to(device)
attention_mask = inputs['attention_mask'].to(device)

# Make the prediction
with torch.no_grad():
    loaded_model.eval()
    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}
    outputs = loaded_model(**inputs)
    logits = outputs.logits

# Get the predicted label
predicted_label = torch.argmax(logits, dim=1).item()
predicted_class = label_encoder.classes_[predicted_label]

print("Predicted Class:", predicted_class)

from IPython.display import display
import ipywidgets as widgets

# Load the saved model's state dictionary
model_path = '/content/model.pt'
loaded_state_dict = torch.load(model_path)

# Create a new instance of the model
loaded_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)

# Load the state dictionary into the model
loaded_model.load_state_dict(loaded_state_dict)

# Set the device to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
loaded_model.to(device)

# Create an input field widget
text_input = widgets.Text(placeholder='Enter your input text', description='Input:')
display(text_input)

# Create a label widget to display the predicted class
output_label = widgets.Label()
display(output_label)

def predict_class(text):
    # Tokenize the input text
    inputs = tokenizer.encode_plus(
        text,
        max_length=64,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    # Make the prediction
    with torch.no_grad():
        loaded_model.eval()
        inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}
        outputs = loaded_model(**inputs)
        logits = outputs.logits

    # Get the predicted label
    predicted_label = torch.argmax(logits, dim=1).item()
    predicted_class = label_encoder.classes_[predicted_label]

    output_label.value = "Predicted Class: " + predicted_class

# Create a button widget
button = widgets.Button(description='Predict')
display(button)

# Define an event handler for the button click
def on_button_click(b):
    text = text_input.value
    predict_class(text)

button.on_click(on_button_click)

